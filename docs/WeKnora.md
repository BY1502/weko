## 소개

WeKnora는 기업 환경에서 즉시 사용 가능한 RAG(검색 증강 생성) 프레임워크로, 지능형 문서 이해 및 검색 기능을 구현합니다. 이 시스템은 모듈식 설계를 채택하여 문서 이해, 벡터 저장, 추론 등의 기능을 분리했습니다.

![arc](./images/arc.png)

---

## 파이프라인 (Pipeline)

WeKnora의 문서 처리는 **삽입 -> 지식 추출 -> 인덱싱 -> 검색 -> 생성**의 여러 단계를 거치며, 전체 프로세스는 다양한 검색 방법을 지원합니다.

![](./images/pipeline2.jpeg)

사용자가 업로드한 '숙박 내역서 PDF 파일'을 예로 들어 데이터 흐름을 상세히 소개하겠습니다.

### 1. 요청 수신 및 초기화

- **요청 식별**: 시스템이 요청을 수신하고 고유한 `request_id=Lkq0OGLYu2fV`를 할당하여 전체 처리 과정을 추적합니다.
- **테넌트 및 세션 검증**:
  - 먼저 테넌트 정보(ID: 1, Name: Default Tenant)를 검증합니다.
  - 이어 지식 베이스 문답(Knowledge QA) 요청 처리를 시작합니다. 이 요청은 세션 `1f241340-ae75-40a5-8731-9a3a82e34fdd`에 속합니다.
- **사용자 질문**: 사용자의 원래 질문은 "**입실한 객실 타입은 무엇인가**"입니다.
- **메시지 생성**: 시스템은 사용자의 질문과 생성될 답변을 위해 각각 메시지 레코드를 생성합니다 (ID: `703ddf09-...` 및 `6f057649-...`).

### 2. 지식 베이스 문답 프로세스 시작

시스템은 정식으로 지식 베이스 문답 서비스를 호출하며, 순차적으로 실행될 9개의 이벤트로 구성된 전체 처리 파이프라인을 정의합니다:
`[rewrite_query, preprocess_query, chunk_search, chunk_rerank, chunk_merge, filter_top_k, into_chat_message, chat_completion_stream, stream_filter]`

---

### 3. 이벤트 실행 상세

#### 이벤트 1: `rewrite_query` - 질문 재작성

- **목적**: 검색의 정확도를 높이기 위해 문맥을 결합하여 사용자의 실제 의도를 파악합니다.
- **작업**:
  1. 현재 세션의 최근 메시지 20개(실제로는 8개 검색됨)를 문맥으로 조회합니다.
  2. `deepseek-r1:7b`라는 로컬 대규모 언어 모델을 호출합니다.
  3. 모델은 대화 기록을 분석하여 질문자가 "Liwx"임을 파악하고, "입실한 객실 타입은 무엇인가"라는 원래 질문을 더 구체적으로 재작성합니다.
- **결과**: 질문이 "**Liwx가 이번에 입실한 객실 타입은 무엇인가**"로 성공적으로 재작성되었습니다.

#### 이벤트 2: `preprocess_query` - 질문 전처리

- **목적**: 재작성된 질문을 형태소 분석(분권)하여 검색 엔진이 처리하기 적합한 키워드 시퀀스로 변환합니다.
- **작업**: 재작성된 질문에 대해 형태소 분석을 수행합니다.
- **결과**: 키워드 시퀀스 생성: "`필요 재작성 사용자 질문 입실 객실타입 근거 제공 정보 입실 자 Liwx 선택 객실타입 트윈 룸 따라서 재작성 후 전체 질문 은 Liwx 이번 입실 객실타입`"

#### 이벤트 3: `chunk_search` - 지식 청크 검색

이것은 가장 핵심적인 **검색(Retrieval)** 단계로, 시스템은 두 번의 하이브리드 검색(Hybrid Search)을 수행합니다.

- **1차 검색 (재작성된 전체 문장 사용)**:
  - **벡터 검색**:
    1. 임베딩 모델 `bge-m3:latest`를 로드하여 문장을 1024차원 벡터로 변환합니다.
    2. PostgreSQL 데이터베이스에서 벡터 유사도 검색을 수행하여 2개의 관련 지식 청크(chunk)를 찾습니다 (ID: `e3bf6599-...`, `3989c6ce-...`).
  - **키워드 검색**:
    1. 동시에 키워드 검색도 수행합니다.
    2. 동일하게 위 2개의 지식 청크를 찾습니다.
  - **결과 병합**: 두 방법으로 찾은 4개의 결과(실제로는 2개 중복)를 중복 제거하여 최종적으로 2개의 고유한 지식 청크를 얻습니다.
- **2차 검색 (전처리된 키워드 시퀀스 사용)**:
  - 시스템은 분권된 키워드를 사용하여 위와 동일한 **벡터 검색**과 **키워드 검색** 과정을 반복합니다.
  - 최종적으로 동일한 2개의 지식 청크를 얻습니다.
- **최종 결과**: 두 번의 검색과 결과 병합을 거쳐, 시스템은 가장 관련성 높은 2개의 지식 청크를 확정하고 내용을 추출하여 답변 생성 준비를 합니다.

#### 이벤트 4: `chunk_rerank` - 결과 재순위화

- **목적**: 더 강력한 모델을 사용하여 1차 검색 결과를 정밀하게 다시 정렬함으로써 최종 답변의 품질을 높입니다.
- **작업**: 로그에 `Rerank model ID is empty, skipping reranking`이 표시됩니다. 이는 재순위화 단계가 구성되었으나 구체적인 모델이 지정되지 않아 **이 단계를 건너뛰었음**을 의미합니다.

#### 이벤트 5: `chunk_merge` - 청크 병합

- **목적**: 내용상 인접하거나 관련된 지식 청크를 병합하여 더 완전한 문맥을 형성합니다.
- **작업**: 시스템은 검색된 2개의 청크를 분석하고 병합을 시도합니다. 로그에 따르면 최종 처리 후에도 여전히 2개의 독립적인 청크로 유지되지만, 관련성 점수에 따라 정렬되었습니다.

#### 이벤트 6: `filter_top_k` - Top-K 필터링

- **목적**: 관련성이 가장 높은 상위 K개의 결과만 유지하여, 너무 많은 무관한 정보가 언어 모델을 방해하는 것을 방지합니다.
- **작업**: 시스템은 상위 5개(Top-K = 5)를 유지하도록 설정되어 있습니다. 현재 2개의 청크만 있으므로 모두 필터를 통과합니다.

#### 이벤트 7 & 8: `into_chat_message` & `chat_completion_stream` - 답변 생성

이것은 **생성(Generation)** 단계입니다.

- **목적**: 검색된 정보를 바탕으로 자연스럽고 매끄러운 답변을 생성합니다.
- **작업**:
  1. 검색된 2개의 지식 청크 내용, 사용자의 원래 질문, 대화 기록을 통합하여 하나의 완전한 프롬프트(Prompt)를 구성합니다.
  2. `deepseek-r1:7b` 대규모 언어 모델을 다시 호출하고, **스트리밍(Stream)** 방식으로 답변 생성을 요청합니다. 스트리밍 출력은 타자기 효과를 구현하여 사용자 경험을 향상시킵니다.

#### 이벤트 9: `stream_filter` - 스트리밍 출력 필터링

- **목적**: 모델이 생성하는 실시간 텍스트 스트림을 후처리하여 불필요한 태그나 내용을 걸러냅니다.
- **작업**:
  - 시스템은 모델의 사고 과정에서 생성될 수 있는 내부 태그(예: `<think>`, `</think>`)를 제거하는 필터를 설정했습니다.
  - 로그를 보면 모델 출력의 첫 번째 토큰이 `<think> 근거`였으며, 필터가 `<think>` 태그를 성공적으로 차단하고 "근거" 및 이후 내용만 전달했습니다.

### 4. 완료 및 응답

- **참조 전송**: 답변 생성과 동시에, 시스템은 근거가 된 2개의 지식 청크를 "참조 내용"으로 프론트엔드에 전송하여 사용자가 출처를 확인할 수 있게 합니다.
- **메시지 업데이트**: 모델이 모든 내용을 생성하면, 시스템은 전체 답변을 이전에 생성된 메시지 레코드(ID: `6f057649-...`)에 업데이트합니다.
- **요청 종료**: 서버가 `200` 성공 상태 코드를 반환하며 이번 질문부터 답변까지의 전체 프로세스가 종료됩니다.

### 요약

이 로그는 전형적인 RAG 프로세스를 완벽하게 기록하고 있습니다. 시스템은 **질문 재작성**과 **전처리**를 통해 사용자 의도를 정확히 파악하고, **벡터 및 키워드 하이브리드 검색**을 통해 지식 베이스에서 관련 정보를 찾습니다. **재순위화**는 건너뛰었지만 **병합**과 **필터링**을 수행했으며, 마지막으로 검색된 지식을 문맥으로 삼아 대규모 언어 모델이 유창하고 정확한 답변을 **생성**하고, **스트리밍 필터링**을 통해 출력의 순수성을 보장했습니다.

## 문서 파싱 및 분할

코드는 독립적인 gRPC 마이크로서비스를 구현하여 문서 내용의 심층 파싱, 분할(Chunking) 및 멀티모달 정보 추출을 전담합니다. 이것이 바로 "비동기 처리" 단계의 핵심 실행자입니다.

### **전체 아키텍처**

이것은 Python 기반의 gRPC 서비스로, 파일(또는 URL)을 수신하여 구조화되고 후속 처리(예: 벡터화) 가능한 텍스트 블록(Chunks)으로 파싱하는 것이 핵심 임무입니다.

- `server.py`: 서비스의 진입점 및 네트워크 계층입니다. 다중 프로세스/스레드 gRPC 서버를 시작하여 Go 백엔드의 요청을 받고 파싱 결과를 반환합니다.
- `parser.py`: 디자인 패턴 중 **퍼사드(Facade) 패턴**입니다. 다양한 구체적인 파서(PDF, DOCX, Markdown 등)의 복잡성을 숨기고 통일된 `Parser` 클래스를 제공합니다. 외부 호출자(`server.py`)는 이 `Parser` 클래스와만 상호작용합니다.
- `base_parser.py`: 파서의 기본 클래스로, 모든 구체적 파서가 공유하는 핵심 로직과 추상 메서드를 정의합니다. 텍스트 분할, 이미지 처리, OCR, 이미지 설명 생성 등 가장 복잡한 기능이 포함된 파싱 프로세스의 "두뇌"입니다.

---

### **상세 작업 흐름**

Go 백엔드가 비동기 작업을 시작하면, 파일 내용과 설정 정보를 가지고 이 Python 서비스에 gRPC 호출을 보냅니다. 전체 처리 과정은 다음과 같습니다:

#### **1단계: 요청 수신 및 배분 (**`server.py`** & **`parser.py`\*\*)

1. **gRPC 서비스 진입 (**`server.py: serve`**)**:
   - `serve()` 함수를 통해 시작됩니다. 환경 변수(`GRPC_WORKER_PROCESSES`, `GRPC_MAX_WORKERS`)에 따라 **다중 프로세스/스레드** 서버를 시작하여 CPU 자원을 활용하고 동시 처리 능력을 높입니다.
2. **요청 처리 (**`server.py: ReadFromFile`**)**:
   - Go 백엔드가 `ReadFromFile` 요청을 보내면 작업 프로세스 중 하나가 이를 수신합니다.
   - 요청 매개변수를 파싱합니다: `file_name`, `file_type`, `file_content` 및 `read_config`(분할 크기, 중첩 크기, 멀티모달 활성화 여부, 스토리지 설정 등).
   - 이 설정들을 `ChunkingConfig` 객체로 통합하고, `self.parser.parse_file(...)`을 호출하여 `Parser` 퍼사드 클래스에 위임합니다.
3. **파서 선택 (**`parser.py: Parser.parse_file`**)**:
   - `Parser`는 `get_parser(file_type)`를 호출하여 파일 유형(예: `'pdf'`)에 맞는 구체적인 파서 클래스(예: `PDFParser`)를 찾습니다.
   - 해당 클래스를 **인스턴스화**하고 모든 설정 정보를 전달합니다.

#### **2단계: 핵심 파싱 및 분할 (**`base_parser.py`**)**

이 부분은 전체 프로세스의 핵심인 **정보의 문맥 완전성과 원래 순서 보장**을 다룹니다.

`base_parser.py` 코드에 따르면, **최종 분할된 Chunk 내의 텍스트, 표, 이미지는 원본 문서에 나타난 순서대로 저장됩니다**.

이 순서는 `BaseParser` 내의 정교한 메서드 협력을 통해 보장됩니다:

1. **단계 1: 통일된 텍스트 스트림 생성 (**`pdf_parser.py`**)**:
   - `parse_into_text` 메서드에서 PDF를 **페이지별로** 처리합니다.
   - 각 페이지 내에서 특정 로직(비표 텍스트 -> 표 -> 이미지 플레이스홀더 순 등)에 따라 모든 내용을 **하나의 긴 문자열**로 연결합니다.
   - **핵심**: 비록 이 단계에서 완벽한 문자 수준의 순서는 아닐지라도, **같은 페이지의 내용이 함께 묶이고** 대략적인 위에서 아래로의 순서를 따릅니다.
   - 모든 페이지 내용은 `"\n\n--- Page Break ---\n\n"`으로 연결되어 **모든 정보(텍스트, Markdown 표, 이미지 플레이스홀더)를 포함한 단일하고 순서 있는 텍스트 스트림 (`final_text`)**이 됩니다.
2. **단계 2: 원자화 및 보호 (**`_split_into_units`**)**:
   - `final_text`는 `_split_into_units` 메서드로 전달됩니다.
   - 이 메서드는 **구조적 무결성을 보장하는 핵심**입니다. 정규 표현식을 사용하여 **전체 Markdown 표**와 **전체 Markdown 이미지 플레이스홀더**를 **분할 불가능한 원자 단위(atomic units)**로 식별합니다.
   - 이 원자 단위들과 그 사이의 일반 텍스트 블록을 `final_text`에 나타난 **원래 순서대로** 리스트(`units`)로 분할합니다.
   - **결과**: `['텍스트', '![...](...)', '텍스트', '|표|...', '텍스트']`와 같이 원본 문서 순서와 완전히 동일한 리스트를 얻습니다.
3. **단계 3: 순차적 분할 (**`chunk_text`**)**:
   - `chunk_text` 메서드는 이 **순서 있는 **`units`** 리스트**를 받습니다.
   - 리스트의 각 단위(`unit`)를 **순서대로** 순회하며 임시 `current_chunk`에 추가하다가 `chunk_size` 상한에 도달하면 저장하고 새 청크를 시작합니다.
   - **핵심**: **`units` 리스트의 순서를 엄격히 따르므로**, 표, 텍스트, 이미지 간의 상대적 순서가 절대 섞이지 않습니다.
4. **단계 4: 이미지 정보 부착 (**`process_chunks_images`**)**:
   - 텍스트 청크가 생성된 후, `process_chunks_images`가 호출됩니다.
   - **각** 청크 내부의 이미지 플레이스홀더를 찾아 AI 처리를 수행합니다.
   - 처리된 이미지 정보(URL, OCR, 캡션 등)를 **해당 Chunk 객체**의 `.images` 속성에 부착합니다.
   - **핵심**: 이 과정은 **Chunk의 순서나 `.content` 내용을 변경하지 않습니다**.

#### **3단계: 멀티모달 처리(활성화 시) (**`base_parser.py`**)**

`enable_multimodal`이 `True`인 경우, 가장 복잡한 멀티모달 처리 단계로 진입합니다.

1. **동시 작업 시작 (**`BaseParser.process_chunks_images`**)**:
   - `asyncio`를 사용하여 **모든 텍스트 청크의 이미지를 동시에 처리**하여 효율성을 극대화합니다.
2. **단일 청크 내 이미지 처리 (**`BaseParser.process_chunk_images_async`**)**:
   - **이미지 참조 추출**: 정규식으로 청크 텍스트 내의 모든 이미지 참조(예: `![alt](image.png)`)를 찾습니다.
   - **이미지 영속화**: 각 이미지에 대해 `download_and_upload_image`를 호출하여 원본 위치에서 이미지를 가져와 **설정된 객체 스토리지(COS/MinIO)에 업로드**하고, 영구적인 URL을 반환받습니다.
   - **동시 AI 처리**: 업로드된 이미지들을 모아 `process_multiple_images`를 호출합니다. `Semaphore`를 통해 동시 처리 수를 제한(예: 5개)하여 과부하를 방지합니다.
3. **단일 이미지 처리 (**`BaseParser.process_image_async`**)**:
   - **OCR**: `perform_ocr`을 호출하여 OCR 엔진(예: `PaddleOCR`)으로 글자를 인식합니다.
   - **이미지 설명(Caption)**: `get_image_caption`을 호출하여 VLM(시각 언어 모델)에 이미지를 보내고 자연어 설명을 생성합니다.
4. **결과 집계**:
   - 처리된 정보(URL, OCR, 캡션)를 해당 `Chunk`의 `.images` 필드에 구조화하여 저장합니다.

#### **4단계: 결과 반환 (**`server.py`**)**

1. **데이터 변환 (**`server.py: _convert_chunk_to_proto`**)**:
   - 파싱이 완료되면 `ParseResult` 리스트가 반환됩니다.
   - `ReadFromFile`은 이를 gRPC Protobuf 메시지 포맷으로 변환합니다.
2. **응답 전송**:
   - gRPC 서버는 최종적으로 `ReadResponse` 메시지를 Go 백엔드로 전송합니다.

이제 Go 백엔드는 구조화되고 풍부한 정보가 담긴 문서 데이터를 확보하게 되며, 다음 단계인 벡터화 및 인덱싱을 진행할 수 있습니다.

## 배포

Docker 이미지 로컬 배포를 지원하며, API 포트를 통해 인터페이스 서비스를 제공합니다.

## 성능 및 모니터링

WeKnora는 풍부한 모니터링 및 테스트 구성 요소를 포함합니다:

- 분산 추적: Jaeger를 통합하여 서비스 아키텍처 내 요청의 전체 실행 경로를 추적합니다. Jaeger는 분산 시스템에서 요청의 수명 주기를 "시각화"하는 기술입니다.
- 건강 모니터링: 서비스가 정상 상태인지 모니터링합니다.
- 확장성: 컨테이너화된 배포를 통해 여러 서비스로 대규모 동시 요청을 처리할 수 있습니다.

## QA (질의응답)

### 질문 1: 검색 과정에서 두 번의 하이브리드 검색을 수행하는 목적은 무엇이며, 1차와 2차 검색의 차이점은 무엇인가요?

매우 좋은 관찰입니다. 두 번의 하이브리드 검색은 **검색의 정확도와 재현율(Recall)을 극대화**하기 위함이며, 본질적으로 **쿼리 확장(Query Expansion)과 다중 전략 검색**의 조합입니다.

#### 목적

서로 다른 형태의 쿼리(원본 재작성 문장 vs 분권된 키워드 시퀀스)로 검색함으로써 두 방식의 장점을 결합합니다:

- **의미론적 검색의 깊이**: 완전한 문장 검색은 벡터 모델(`bge-m3` 등)의 문장 이해 능력을 활용하여 의미상 가장 가까운 지식을 찾습니다.
- **키워드 검색의 넓이**: 분권된 키워드 검색은 지식 청크의 표현 방식이 다르더라도 핵심 단어가 포함되어 있다면 검색되도록 보장합니다. 이는 전통적인 키워드 매칭(BM25 등)에 효과적입니다.

간단히 말해, **같은 질문을 두 가지 다른 방식으로 물어보고** 결과를 합쳐 누락을 방지하는 것입니다.

#### 두 검색의 차이점

핵심 차이는 **입력 쿼리 텍스트(Query Text)**입니다:

1. **1차 하이브리드 검색**

   - **입력**: `rewrite_query` 이벤트를 거쳐 생성된, **문법적으로 완전한 자연어 문장**.
   - **로그 예시**:
     ```plain
     INFO ... query text: ... 개작된 전체 질문은: "Liwx가 이번에 입실한 객실 타입은 무엇인가"
     ```

2. **2차 하이브리드 검색**
   - **입력**: `preprocess_query` 이벤트를 거쳐 생성된, **공백으로 구분된 키워드 시퀀스**.
   - **로그 예시**:
     ```plain
     INFO ... query text: 필요 재작성 사용자 질문 입실 객실타입 ... Liwx 이번 입실 객실타입
     ```

최종적으로 시스템은 이 두 검색 결과를 중복 제거하고 병합하여 더 신뢰할 수 있는 지식 집합을 얻습니다.

### 질문 2: 재순위화(Rerank) 모델 분석

Reranker는 RAG 분야의 고급 기술로, 작동 원리와 적용 시나리오에 따라 차이가 있습니다.

#### 1. Normal Reranker (일반 재순위화기 / Cross-Encoder)

가장 고전적이고 주류인 방식입니다.

- **모델 유형**: 시퀀스 분류 모델. (BERT 기반 Cross-Encoder 등). `BAAI/bge-reranker-v2-m3` 등이 해당됩니다.
- **원리**: 쿼리와 문서를 하나의 시퀀스로 연결하여 모델에 입력하고, 모델이 두 텍스트 간의 상호작용을 계산하여 **단일 관련성 점수**를 출력합니다.
- **장점**: 정확도가 매우 높습니다.
- **단점**: 속도가 느립니다. (모든 문서 쌍에 대해 계산 필요)

#### 2. LLM-based Reranker (LLM 기반)

범용 LLM의 능력을 활용합니다.

- **원리**: 재순위화 작업을 "이 문서가 질문에 대한 답을 포함하는가?"라는 질문에 대해 "Yes/No"를 생성하는 작업으로 변환합니다. "Yes"가 생성될 확률을 관련성 점수로 사용합니다.
- **장점**: 복잡한 추론이 필요한 쿼리에 유리합니다.
- **단점**: 계산 비용이 크고 프롬프트에 민감합니다.

#### 3. LLM-based Layerwise Reranker (LLM 계층 기반)

2번 방식의 심화 버전입니다.

- **원리**: LLM의 마지막 레이어뿐만 아니라 **중간 레이어들**에서 "Yes" 예측 값을 추출하여 종합합니다.
- **장점**: 더 풍부한 관련성 신호를 얻을 수 있어 이론상 정확도가 가장 높습니다.
- **단점**: 가장 복잡하고 리소스 소모가 큽니다.

#### 사용 제안

1. **시작 단계**: `Normal Reranker` (예: `bge-reranker-v2-m3`)로 시작하는 것을 강력히 권장합니다. 성능과 속도의 균형이 가장 좋습니다.
2. **고급 탐색**: 미묘한 추론이 필요한 경우 `LLM-based Reranker`를 시도해 볼 수 있습니다.

### 질문 3: 필터링된 지식을 대규모 모델에 어떻게 전송하나요?

이 부분은 프롬프트 엔지니어링의 영역입니다. 컨텍스트를 조립할 때 다음을 명시해야 합니다:

- **핵심 제약**: 반드시 제공된 문서에 근거하여 답변할 것. 자신의 지식을 사용하지 말 것.
- **미지 처리**: 문서에 정보가 없으면 "제공된 자료로는 답변할 수 없습니다"라고 할 것.
- **인용 요구**: 답변 시 문서 내용을 인용했다면 문장 끝에 문서 번호를 붙일 것.

---

## 수동 지식 온라인 편집

지식 베이스 페이지에 "문서 업로드 / 온라인 편집" 입구가 추가되어 브라우저에서 직접 Markdown 지식을 작성하고 유지 관리할 수 있습니다:

- 초안 모드는 저장을 위한 것이며 검색에 참여하지 않습니다.
- 게시(Publish) 작업은 자동으로 벡터화 및 인덱스 구축을 트리거합니다.
- 게시된 Markdown 지식은 다시 열어 수정하고 재게시할 수 있습니다.
- 대화 페이지의 어시스턴트 답변 끝에 "지식 베이스에 추가" 도구를 제공하여, 현재 문답을 에디터로 가져와 확인 후 저장할 수 있습니다.
